{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "biuZzd2W_a6X"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import sys, os, gzip, shutil, math\n",
        "!{sys.executable} -m pip install openneuro-py nibabel scikit-learn seaborn tqdm --quiet\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as T\n",
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, f1_score, classification_report, roc_curve\n",
        "from collections import Counter, defaultdict\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import statistics as stats\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "\n",
        "#  OpenNeuro\n",
        "\n",
        "dataset_root_path = \"ds003463\"\n",
        "if not os.path.exists(dataset_root_path):\n",
        "    os.system(f\"openneuro-py download --dataset={dataset_root_path}\")\n",
        "else:\n",
        "    print(\"Dossier déjà présent, téléchargement sauté.\")\n",
        "\n",
        "\n",
        "#prepa données\n",
        "\n",
        "label_map = {\n",
        "    'sub-m01': 1, 'sub-m02': 1, 'sub-m03': 1, 'sub-m04': 1, 'sub-m05': 1,\n",
        "    'sub-m06': 1, 'sub-m07': 1, 'sub-m08': 1, 'sub-m09': 0, 'sub-m10': 0,\n",
        "    'sub-m11': 0, 'sub-m12': 0, 'sub-m13': 0, 'sub-m14': 0, 'sub-r01': 1,\n",
        "    'sub-r02': 1, 'sub-r03': 1, 'sub-r04': 1, 'sub-r05': 0, 'sub-r06': 0,\n",
        "    'sub-r07': 0, 'sub-r08': 0\n",
        "}\n",
        "\n",
        "def is_valid_nifti(path: str) -> bool:\n",
        "    try:\n",
        "        img = nib.load(path, mmap=False)\n",
        "        _ = img.get_fdata(dtype=np.float32)  # force la lecture\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def subject_from_path(p: str) -> str:\n",
        "    parts = p.replace(\"\\\\\", \"/\").split(\"/\")\n",
        "    for i, token in enumerate(parts):\n",
        "        if token.startswith(\"sub-\"):\n",
        "            return token\n",
        "    return \"unknown\"\n",
        "\n",
        "all_files, all_labels, all_subjects = [], [], []\n",
        "for sub in label_map.keys():\n",
        "    sub_path = os.path.join(dataset_root_path, sub)\n",
        "    if not os.path.isdir(sub_path):\n",
        "        continue\n",
        "    for ses in os.listdir(sub_path):\n",
        "        if not ses.startswith(\"ses-\"):\n",
        "            continue\n",
        "        anat = os.path.join(sub_path, ses, \"anat\")\n",
        "        if not os.path.isdir(anat):\n",
        "            continue\n",
        "        for f in os.listdir(anat):\n",
        "            if \"MGE\" in f and f.endswith(\".nii.gz\"):\n",
        "                fullp = os.path.join(anat, f)\n",
        "                if is_valid_nifti(fullp):\n",
        "                    all_files.append(fullp)\n",
        "                    all_subjects.append(sub)\n",
        "                    all_labels.append(label_map[sub])\n",
        "\n",
        "print(f\"IRMs valides: {len(all_files)} fichiers, {len(set(all_subjects))} sujets.\")\n",
        "\n",
        "\n",
        "# Split stratif par SJT\n",
        "\n",
        "print(\"\\n### ÉTAPE 4: Split train/test stratifié par SUJET ###\")\n",
        "\n",
        "labels_np = np.array(all_labels)\n",
        "subjects_np = np.array(all_subjects)\n",
        "idxs = np.arange(len(all_files))\n",
        "\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "train_idx, test_idx = next(sgkf.split(idxs, labels_np, groups=subjects_np))\n",
        "\n",
        "train_files = [all_files[i] for i in train_idx]\n",
        "train_labels = [all_labels[i] for i in train_idx]\n",
        "train_subjects = [all_subjects[i] for i in train_idx]\n",
        "\n",
        "test_files  = [all_files[i] for i in test_idx]\n",
        "test_labels = [all_labels[i] for i in test_idx]\n",
        "test_subjects = [all_subjects[i] for i in test_idx]\n",
        "\n",
        "print(f\"Train: {len(train_files)} fichiers / {len(set(train_subjects))} sujets\")\n",
        "print(f\"Test : {len(test_files)} fichiers / {len(set(test_subjects))} sujets\")\n",
        "\n",
        "\n",
        "def robust_normalize(vol: np.ndarray):\n",
        "    if vol.ndim == 4:\n",
        "        vol = vol.mean(axis=3)\n",
        "    lo, hi = np.percentile(vol, [0.5, 99.5])\n",
        "    vol = np.clip(vol, lo, hi)\n",
        "    mu, sd = vol.mean(), vol.std() + 1e-8\n",
        "    return (vol - mu) / sd\n",
        "\n",
        "def make_triplet(vol: np.ndarray, k: int, mode=\"center\"):\n",
        "\n",
        "    assert vol.ndim == 3\n",
        "    z = vol.shape[2]\n",
        "    mids = []\n",
        "    if mode == \"center\":\n",
        "        mid = z // 2\n",
        "\n",
        "        half = k // 2\n",
        "        start = max(0, mid - half)\n",
        "        idxs = np.arange(start, min(start + k, z))\n",
        "    else:\n",
        "\n",
        "        idxs = np.linspace(z*0.35, z*0.65, num=k).astype(int)\n",
        "\n",
        "    triplets = []\n",
        "    for m in idxs:\n",
        "        i1 = max(0, m-1); i2 = m; i3 = min(z-1, m+1)\n",
        "        arr = np.stack([vol[:,:,i1], vol[:,:,i2], vol[:,:,i3]], axis=0).astype(np.float32)\n",
        "        triplets.append(arr)\n",
        "    return triplets\n",
        "\n",
        "class MRITripletDataset(Dataset):\n",
        "    def __init__(self, file_paths, labels, subjects, k_slices=5, train=True):\n",
        "        self.samples = []\n",
        "        self.train = train\n",
        "        self.k = k_slices\n",
        "\n",
        "        # Transf\n",
        "        if train:\n",
        "            self.tf = T.Compose([\n",
        "                T.Resize((224,224), antialias=True),\n",
        "                T.RandomRotation(degrees=10),\n",
        "                T.RandomHorizontalFlip(p=0.5),\n",
        "                T.Normalize(mean=[0.0,0.0,0.0], std=[1.0,1.0,1.0]),\n",
        "            ])\n",
        "        else:\n",
        "            self.tf = T.Compose([\n",
        "                T.Resize((224,224), antialias=True),\n",
        "                T.Normalize(mean=[0.0,0.0,0.0], std=[1.0,1.0,1.0]),\n",
        "            ])\n",
        "\n",
        "        for p, y, s in tqdm(list(zip(file_paths, labels, subjects)),\n",
        "                            total=len(file_paths),\n",
        "                            desc=(\"Génération triplets [TRAIN]\" if train else \"Génération triplets [TEST]\")):\n",
        "            vol = nib.load(p, mmap=False).get_fdata(dtype=np.float32)\n",
        "            vol = robust_normalize(vol)\n",
        "            trips = make_triplet(vol, k=self.k, mode=\"center\")\n",
        "            for arr in trips:\n",
        "                x = torch.from_numpy(arr)\n",
        "                x = self.tf(x)\n",
        "                self.samples.append((x, int(y), s, p))\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, i):\n",
        "        x, y, subj, path = self.samples[i]\n",
        "        return x, torch.tensor(y, dtype=torch.long), subj, path\n",
        "\n",
        "k_slices = 5\n",
        "train_ds = MRITripletDataset(train_files, train_labels, train_subjects, k_slices=k_slices, train=True)\n",
        "test_ds  = MRITripletDataset(test_files,  test_labels,  test_subjects,  k_slices=k_slices, train=False)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=0)\n",
        "test_loader  = DataLoader(test_ds,  batch_size=64, shuffle=False, num_workers=0)\n",
        "\n",
        "print(f\"Train samples (triplets): {len(train_ds)}\")\n",
        "print(f\"Test  samples (triplets): {len(test_ds)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CV 2.5D Resnet\n",
        "#\n",
        "def build_datasets(train_files, train_labels, train_subjects,\n",
        "                   test_files,  test_labels,  test_subjects,\n",
        "                   k_slices=5):\n",
        "    train_ds = MRITripletDataset(train_files, train_labels, train_subjects,\n",
        "                                 k_slices=k_slices, train=True)\n",
        "    test_ds  = MRITripletDataset(test_files,  test_labels,  test_subjects,\n",
        "                                 k_slices=k_slices, train=False)\n",
        "    train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=0)\n",
        "    test_loader  = DataLoader(test_ds,  batch_size=64, shuffle=False, num_workers=0)\n",
        "    return train_ds, test_ds, train_loader, test_loader\n",
        "\n",
        "def make_model(device):\n",
        "    m = models.resnet18(weights='IMAGENET1K_V1')\n",
        "    m.fc = nn.Linear(m.fc.in_features, 2)\n",
        "    m = m.to(device)\n",
        "    for p in m.parameters(): p.requires_grad = False\n",
        "    for p in m.fc.parameters(): p.requires_grad = True\n",
        "    return m\n",
        "\n",
        "def train_one_fold(model, train_loader, test_loader, class_weights, device,\n",
        "                   warmup_epochs=3, max_epochs=30):\n",
        "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                            lr=3e-4, weight_decay=1e-4)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                     factor=0.5, patience=2)\n",
        "\n",
        "    def eval_val_loss():\n",
        "        model.eval()\n",
        "        loss_sum, n = 0.0, 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb, _, _ in test_loader:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                loss_sum += criterion(model(xb), yb).item() * xb.size(0)\n",
        "                n += xb.size(0)\n",
        "        return loss_sum / max(1, n)\n",
        "\n",
        "    best_val = float('inf'); patience, no_imp = 6, 0\n",
        "    for epoch in range(1, max_epochs+1):\n",
        "        model.train()\n",
        "        for xb, yb, _, _ in tqdm(train_loader, desc=f\"Fold train epoch {epoch}/{max_epochs}\", leave=False):\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(xb), yb)\n",
        "            loss.backward(); optimizer.step()\n",
        "        val_loss = eval_val_loss(); scheduler.step(val_loss)\n",
        "\n",
        "        if epoch == warmup_epochs:\n",
        "            for name, m in model.named_children():\n",
        "                if name in [\"layer3\", \"layer4\", \"bn1\", \"conv1\", \"fc\"]:\n",
        "                    for p in m.parameters(): p.requires_grad = True\n",
        "            optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                                    lr=1e-4, weight_decay=1e-4)\n",
        "\n",
        "        if val_loss < best_val - 1e-4:\n",
        "            best_val = val_loss; no_imp = 0\n",
        "            torch.save(model.state_dict(), \"best_fold.pt\")\n",
        "        else:\n",
        "            no_imp += 1\n",
        "            if no_imp >= patience:\n",
        "                print(\"  early stopping.\")\n",
        "                break\n",
        "\n",
        "    model.load_state_dict(torch.load(\"best_fold.pt\", map_location=device))\n",
        "    return model\n",
        "\n",
        "def evaluate_slice_and_subject(model, test_loader):\n",
        "    model.eval()\n",
        "    sm = nn.Softmax(dim=1)\n",
        "    all_probs, all_preds, all_true = [], [], []\n",
        "    all_subjects, all_paths = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb, subj, path in test_loader:\n",
        "            xb = xb.to(device)\n",
        "            probs = sm(model(xb)).cpu().numpy()\n",
        "            preds = probs.argmax(1)\n",
        "            all_probs.append(probs[:,1]); all_preds.append(preds); all_true.append(yb.numpy())\n",
        "            all_subjects.extend(list(subj)); all_paths.extend(list(path))\n",
        "    all_probs = np.concatenate(all_probs); all_preds = np.concatenate(all_preds); all_true = np.concatenate(all_true)\n",
        "\n",
        "    # par cp\n",
        "    acc_slice = accuracy_score(all_true, all_preds)\n",
        "    f1_slice  = f1_score(all_true, all_preds)\n",
        "    try: roc_slice = roc_auc_score(all_true, all_probs)\n",
        "    except: roc_slice = float(\"nan\")\n",
        "\n",
        "    # par sjt\n",
        "    by_subject_scores = defaultdict(list); by_subject_true = {}\n",
        "    for p1, y, s in zip(all_probs, all_true, all_subjects):\n",
        "        by_subject_scores[s].append(p1); by_subject_true[s] = y\n",
        "    subj_probs, subj_true, subj_pred = [], [], []\n",
        "    for s, scores in by_subject_scores.items():\n",
        "        p = float(np.mean(scores)); y = by_subject_true[s]; pred = int(p >= 0.5)\n",
        "        subj_probs.append(p); subj_true.append(y); subj_pred.append(pred)\n",
        "    subj_probs = np.array(subj_probs); subj_true = np.array(subj_true); subj_pred = np.array(subj_pred)\n",
        "\n",
        "    acc_subj = accuracy_score(subj_true, subj_pred)\n",
        "    f1_subj  = f1_score(subj_true, subj_pred)\n",
        "    try: roc_subj = roc_auc_score(subj_true, subj_probs)\n",
        "    except: roc_subj = float(\"nan\")\n",
        "\n",
        "    return {\n",
        "        \"slice\": {\"acc\": acc_slice, \"f1\": f1_slice, \"roc\": roc_slice},\n",
        "        \"subject\": {\"acc\": acc_subj, \"f1\": f1_subj, \"roc\": roc_subj},\n",
        "        \"n_subjects_test\": len(subj_true)\n",
        "    }\n",
        "\n",
        "# les listes\n",
        "files = all_files\n",
        "labels = np.array(all_labels)\n",
        "subjects = np.array(all_subjects)\n",
        "idxs = np.arange(len(files))\n",
        "\n",
        "# Cross-validation 5-fold par sujet\n",
        "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "fold_results = []\n",
        "fold_id = 0\n",
        "for train_idx, test_idx in sgkf.split(idxs, labels, groups=subjects):\n",
        "    fold_id += 1\n",
        "    print(f\"\\n========== FOLD {fold_id} ==========\")\n",
        "    tr_files = [files[i] for i in train_idx]\n",
        "    tr_labels = [int(labels[i]) for i in train_idx]\n",
        "    tr_subjects = [subjects[i] for i in train_idx]\n",
        "    te_files = [files[i] for i in test_idx]\n",
        "    te_labels = [int(labels[i]) for i in test_idx]\n",
        "    te_subjects = [subjects[i] for i in test_idx]\n",
        "\n",
        "    print(f\"Train: {len(set(tr_subjects))} sujets, Test: {len(set(te_subjects))} sujets\")\n",
        "\n",
        "\n",
        "    counts = Counter(tr_labels); total = sum(counts.values())\n",
        "    w0 = total/(2.0*counts[0]); w1 = total/(2.0*counts[1])\n",
        "    class_weights = torch.tensor([w0, w1], dtype=torch.float, device=device)\n",
        "\n",
        "    # Data\n",
        "    train_ds, test_ds, train_loader, test_loader = build_datasets(\n",
        "        tr_files, tr_labels, tr_subjects,\n",
        "        te_files, te_labels, te_subjects,\n",
        "        k_slices=5\n",
        "    )\n",
        "\n",
        "    # Modele + entrainement\n",
        "    model = make_model(device)\n",
        "    model = train_one_fold(model, train_loader, test_loader, class_weights, device,\n",
        "                           warmup_epochs=3, max_epochs=30)\n",
        "\n",
        "    # evaluation\n",
        "    metrics = evaluate_slice_and_subject(model, test_loader)\n",
        "    fold_results.append(metrics)\n",
        "\n",
        "    s = metrics[\"slice\"]; u = metrics[\"subject\"]\n",
        "    print(f\"[FOLD {fold_id}] Par coupe  : Acc={s['acc']*100:.1f}% | F1={s['f1']:.3f} | ROC-AUC={s['roc']:.3f}\")\n",
        "    print(f\"[FOLD {fold_id}] PAR SUJET : Acc={u['acc']*100:.1f}% | F1={u['f1']:.3f} | ROC-AUC={u['roc']:.3f} | n_test_sujets={metrics['n_subjects_test']}\")\n",
        "\n",
        "#\n",
        "def mean_std(vals):\n",
        "    vals = [v for v in vals if not (isinstance(v, float) and (np.isnan(v) or np.isinf(v)))]\n",
        "    return (np.mean(vals), np.std(vals)) if vals else (float(\"nan\"), float(\"nan\"))\n",
        "\n",
        "slice_acc = [fr[\"slice\"][\"acc\"] for fr in fold_results]\n",
        "slice_f1  = [fr[\"slice\"][\"f1\"]  for fr in fold_results]\n",
        "slice_roc = [fr[\"slice\"][\"roc\"] for fr in fold_results]\n",
        "\n",
        "subj_acc = [fr[\"subject\"][\"acc\"] for fr in fold_results]\n",
        "subj_f1  = [fr[\"subject\"][\"f1\"]  for fr in fold_results]\n",
        "subj_roc = [fr[\"subject\"][\"roc\"] for fr in fold_results]\n",
        "\n",
        "m_sa, s_sa = mean_std(slice_acc)\n",
        "m_sf, s_sf = mean_std(slice_f1)\n",
        "m_sr, s_sr = mean_std(slice_roc)\n",
        "\n",
        "m_ua, s_ua = mean_std(subj_acc)\n",
        "m_uf, s_uf = mean_std(subj_f1)\n",
        "m_ur, s_ur = mean_std(subj_roc)\n",
        "\n",
        "print(\"\\n========== RÉSUMÉ CROSS-VAL ==========\")\n",
        "print(f\"Par coupe  : Acc={m_sa*100:.1f}±{s_sa*100:.1f}% | F1={m_sf:.3f}±{s_sf:.3f} | ROC-AUC={m_sr:.3f}±{s_sr:.3f}\")\n",
        "print(f\"PAR SUJET : Acc={m_ua*100:.1f}±{s_ua*100:.1f}% | F1={m_uf:.3f}±{s_uf:.3f} | ROC-AUC={m_ur:.3f}±{s_ur:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRzFQTVjIBHQ",
        "outputId": "1ad6e6e8-43fb-400a-fdef-2f7dd69f1012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== FOLD 1 ==========\n",
            "Train: 17 sujets, Test: 4 sujets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Génération triplets [TRAIN]: 100%|██████████| 268/268 [00:45<00:00,  5.95it/s]\n",
            "Génération triplets [TEST]: 100%|██████████| 75/75 [00:09<00:00,  8.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  early stopping.\n",
            "[FOLD 1] Par coupe  : Acc=60.5% | F1=0.357 | ROC-AUC=0.539\n",
            "[FOLD 1] PAR SUJET : Acc=75.0% | F1=0.000 | ROC-AUC=0.667 | n_test_sujets=4\n",
            "\n",
            "========== FOLD 2 ==========\n",
            "Train: 17 sujets, Test: 4 sujets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Génération triplets [TRAIN]: 100%|██████████| 283/283 [00:49<00:00,  5.73it/s]\n",
            "Génération triplets [TEST]: 100%|██████████| 60/60 [00:04<00:00, 13.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  early stopping.\n",
            "[FOLD 2] Par coupe  : Acc=62.0% | F1=0.748 | ROC-AUC=0.533\n",
            "[FOLD 2] PAR SUJET : Acc=75.0% | F1=0.857 | ROC-AUC=0.667 | n_test_sujets=4\n",
            "\n",
            "========== FOLD 3 ==========\n",
            "Train: 18 sujets, Test: 3 sujets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Génération triplets [TRAIN]: 100%|██████████| 304/304 [00:47<00:00,  6.39it/s]\n",
            "Génération triplets [TEST]: 100%|██████████| 39/39 [00:06<00:00,  5.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  early stopping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[FOLD 3] Par coupe  : Acc=92.8% | F1=0.963 | ROC-AUC=nan\n",
            "[FOLD 3] PAR SUJET : Acc=100.0% | F1=1.000 | ROC-AUC=nan | n_test_sujets=3\n",
            "\n",
            "========== FOLD 4 ==========\n",
            "Train: 16 sujets, Test: 5 sujets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Génération triplets [TRAIN]: 100%|██████████| 250/250 [00:35<00:00,  7.01it/s]\n",
            "Génération triplets [TEST]: 100%|██████████| 93/93 [00:17<00:00,  5.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  early stopping.\n",
            "[FOLD 4] Par coupe  : Acc=49.5% | F1=0.319 | ROC-AUC=0.452\n",
            "[FOLD 4] PAR SUJET : Acc=40.0% | F1=0.000 | ROC-AUC=0.333 | n_test_sujets=5\n",
            "\n",
            "========== FOLD 5 ==========\n",
            "Train: 16 sujets, Test: 5 sujets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Génération triplets [TRAIN]: 100%|██████████| 267/267 [00:42<00:00,  6.30it/s]\n",
            "Génération triplets [TEST]: 100%|██████████| 76/76 [00:11<00:00,  6.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  early stopping.\n",
            "[FOLD 5] Par coupe  : Acc=37.9% | F1=0.330 | ROC-AUC=0.338\n",
            "[FOLD 5] PAR SUJET : Acc=40.0% | F1=0.400 | ROC-AUC=0.167 | n_test_sujets=5\n",
            "\n",
            "========== RÉSUMÉ CROSS-VAL ==========\n",
            "Par coupe  : Acc=60.5±18.3% | F1=0.543±0.264 | ROC-AUC=0.466±0.082\n",
            "PAR SUJET : Acc=66.0±23.1% | F1=0.451±0.419 | ROC-AUC=0.458±0.217\n"
          ]
        }
      ]
    }
  ]
}